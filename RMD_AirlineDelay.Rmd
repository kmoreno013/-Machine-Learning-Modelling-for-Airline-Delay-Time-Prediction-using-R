---
title: 'ML Modelling for Airline Delay Time Prediction'
author: "Kendrick Moreno"
date: "2024-06-04"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## The Problem

Assume that you’re working with an airline customer named Amy and she wants to choose a flight from *Los Angeles to New York* for a business trip. She has a tight schedule, so she wants to make sure her flight won't be delayed. But the problem is, she needs to predict the flight delay rate so she can decide which flight to book.

This report will use the Airline Reporting Carrier On-Time Performance dataset which comes from the Data Asset eXchange website of IBM.

I performed the prescribed data analysis methodologies to analyze the data and to provide resolution to the problem which is indicated below:

-   Data Wrangling

-   Explanatory Data Analysis

-   Prediction Model Development

-   Prediction Model Evaluation

```{r libraries, include=FALSE}
library(tidyverse)
library(knitr)
library(lubridate)
library(corrplot)
library(broom)
library(tibble)
library(purrr)
library(tidymodels)
library(Metrics)
library(parsnip)
library(rsconnect)
```

## Data Wrangling

For this section, we are going to do the following steps to make the data ready for analysis

1.  Missing Values and Formatting (Identify Missing Values, Handle Missing Values and Correct Data Format)
2.  Data Normalization (Simple Scaling, Min-max and Standardization (Z-score))
3.  Binning
4.  Indicator Variable

### Missing Values and Formatting

Now lets retrieve first the raw data from the IBM Data Asset Exchange Site.

```{r download file, message=FALSE, warning=FALSE, include=FALSE}
# url where the data is located
url <- "https://dax-cdn.cdn.appdomain.cloud/dax-airline/1.0.1/lax_to_jfk.tar.gz"

# download the file
download.file(url, destfile = "lax_to_jfk.tar.gz")

# untar the file so we can get the csv only
# if you run this on your local machine, then can remove tar = "internal" 
untar("lax_to_jfk.tar.gz", tar = "internal")

# read_csv only  and store it in a data frame
sub_airline <- read_csv("lax_to_jfk/lax_to_jfk.csv",
                     col_types = cols('DivDistance' = col_number(), 
                                      'DivArrDelay' = col_number()))
```

#### Identify Missing Values

Now let's check the loaded data from the *sub_airline* data frame.

**Table 1:** List of values of Airline data

```{r quick check, echo=FALSE}
output <- capture.output(str(sub_airline))
info <- output[grep(":", output)]
info_split <- strsplit(info, ": ")
output_table <- data.frame(
  Attribute = sapply(info_split, `[`, 1),
  Value = sapply(info_split, `[`, 2)
)
print(output_table)
```

As we can see there are records that have "NA", lets properly check all columns in the data frame with NA values.

**Table 2:** List of values with NA

```{r mapNA, echo=FALSE}
str_output <- capture.output(str(sub_airline))

# Capture the output of map()
map_output <- map(sub_airline, ~sum(is.na(.)))

# Extract relevant information from str()
str_info <- str_output[grep(":", str_output)]
str_info_split <- strsplit(str_info, ": ")
str_output_table <- data.frame(
  Attribute = sapply(str_info_split, `[`, 1),
  Value = sapply(str_info_split, `[`, 2)
)

# Ensure map_output has the same length as str_output_table
if (length(map_output) < nrow(str_output_table)) {
  map_output <- c(map_output, rep(NA, nrow(str_output_table) - length(map_output)))
}

# Combine the information from str() and map()
combined_table <- cbind(str_output_table, NA_count = unlist(map_output))

# Print the combined table
print(combined_table)
```

Now let's check if there are columns that have missing data.

**Table 3:** Count of columns with missing data

```{r dimdata, echo=FALSE}
# Check dimensions of the dataset
dim(sub_airline)
```

Based on the summary above, *"CarrierDelay", "WeatherDelay", "NASDelay", "SecurityDelay" and "LateAircraftDelay"* columns have 2486 rows of missing data, while *"DivDistance" and "DivArrDelay"* columns have 2855 rows of missing data. All other columns do not have missing data

#### Handle Missing Values

Typically, we should not blindly drop NAs but if an entire column or almost the entire column contains NAs, then it may be a good idea to leave it out. In our dataset, columns *DivDistance* and *DivArrDelay* are nearly all empty so we will drop them entirely.

```{r dropdata, echo=FALSE, paged.print=TRUE}
drop_na_cols <- sub_airline %>% select(-DivDistance, -DivArrDelay)
```

We see *CarrierDelay, WeatherDelay, NASDelay, SecurityDelay, LateAircraftDelay* have the same amount of missing values from the summary. By dropping the missing values in one column will also solve the missing value issues in the others.

```{r droprow, echo=FALSE}
pre_airlinedf <- drop_na_cols %>% drop_na(CarrierDelay, WeatherDelay, NASDelay, SecurityDelay, LateAircraftDelay)
```

Then lets replace the value of NA for columns with accepted values using the mean value of the row.

```{r convertNA, include=FALSE}

# Replace the missing values in five columns
pre_airlinedf <- pre_airlinedf %>% replace_na(list(CarrierDelay = 0,
                                              WeatherDelay = 0,
                                              NASDelay = 0,
                                              SecurityDelay = 0,
                                              LateAircraftDelay = 0))

carrier_mean <- mean(pre_airlinedf$CarrierDelay)
pre_airlinedf %>% replace_na(list(CarrierDelay = carrier_mean))

Weather_mean <- mean(pre_airlinedf$WeatherDelay)
pre_airlinedf %>% replace_na(list(WeatherDelay = Weather_mean))

NAS_mean <- mean(pre_airlinedf$NASDelay)
pre_airlinedf %>% replace_na(list(NASDelay = NAS_mean))

Security_mean <- mean(pre_airlinedf$SecurityDelay)
pre_airlinedf %>% replace_na(list(SecurityDelay = Security_mean))

LateAircraft_mean <- mean(pre_airlinedf$LateAircraftDelay)
pre_airlinedf %>% replace_na(list(LateAircraftDelay = LateAircraft_mean))
```

After all the pre-cleaning of data, the dataset has no missing and NA values.

#### Correct Data Format

Now lets check if all fields have the correct data type.

**Table 4:** List of columns with its data type

```{r checktype, echo=FALSE}
pre_airlinedf %>% 
    summarize_all(class) %>% 
    gather(variable, class)
```

Then reformatting the FlightDate field into three separate fields (year, month, day) using separate().

```{r updatedtype, message=FALSE, warning=FALSE, include=FALSE}
pre_airlinedf <- replace_na(pre_airlinedf) %>% 
    separate(FlightDate, sep = "-", into = c("year", "month", "day"))

pre_airlinedf %>%
    select(year, month, day) %>%
    mutate_all(type.convert) %>%
    mutate_if(is.character, as.numeric)
```

After all the transformation of data, the dataset will not have missing values and all fields in its proper format.

### Data Normalization

Now lets perform the process of transforming values of several features (variables) into a similar range.

#### Scaling

There are 3 scaling techniques which are the simple scaling, min-max and data standardization. We are going to perform the data standardization. **Standardization (Z-score)** subtracts the mean (μ) of the feature and divides by the standard deviation (σ). xnew=xold−μσ

**Table 5:** Data standardization using Z-score

```{r scalez, echo=FALSE}
z_scale_airlinedf <- (pre_airlinedf$ArrDelay - mean(pre_airlinedf$ArrDelay)) / sd(pre_airlinedf$ArrDelay)
head({z_scale_airlinedf})
```

#### Binning

Now lets transform continuous numerical variables into discrete categorical 'bins', for grouped analysis then plot a histogram of flight arrival delays, to see what the distribution of "ArrDelay" looks like.

**Image 1:** Distribution of Quantile Rank of the ArrDelay data

```{r binning, echo=FALSE, fig.align='center'}
bin_airlinedf <- pre_airlinedf %>%
      mutate(quantile_rank = ntile(pre_airlinedf$ArrDelay,3))

ggplot(data = bin_airlinedf, mapping = aes(x = quantile_rank)) +
  geom_histogram(bins = 3, color = "white", fill = "red")
```

#### Indicator Variable

Regression models need numerical variables, however categorical variables in their original forms are strings. Creating an Indicator Variable to label numerical variable used to label categories.

```{r indvar1, include=FALSE}
pre_airlinedf %>% 
  spread(Reporting_Airline, ArrDelay) %>% 
  slice(1:5)
```

**Image 2:** Data points illustration for each Airline

```{r indvar2, echo=FALSE, fig.align='center'}
pre_airlinedf %>%
  mutate(Reporting_Airline = factor(Reporting_Airline,
                                    levels = c("AA", "AS", "DL", "UA", "B6", "PA (1)", "HP", "TW", "VX"),
                                    labels = c("American Airlines", "Alaska Airlines", "Delta Air Lines", "United Airlines", "JetBlue Airways", "Pan American World Airways", "Hawaiian Airlines", "Trans World Airlines", "Virgin America"))) %>%
  ggplot(aes(x = Reporting_Airline, fill = Reporting_Airline)) +
  stat_count(width = 0.5) +
  labs(x = "Airlines", y = "Number of data points in each airline") +
  scale_fill_manual(values = c("American Airlines" = "red", 
                                "Alaska Airlines" = "blue", 
                                "Delta Air Lines" = "green",
                                "United Airlines" = "orange",
                                "JetBlue Airways" = "purple",
                                "Pan American World Airways" = "yellow",
                                "Hawaiian Airlines" = "cyan",
                                "Trans World Airlines" = "magenta",
                                "Virgin America" = "black"),
                     name = "Airlines") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  coord_flip()  # Flip the coordinates
```

## Exploratory Data Analysis

For this section, we are going to do the following steps to analyze the data before performing data modelling.

1.  Analyzing Individual Feature Patterns using Visualization
2.  Descriptive Statistical Analysis
3.  Basics of Grouping
4.  Correlation and Causation
5.  ANOVA

### Analyzing Individual Feature Patterns using Visualization

To analyze the data, lets prepare a boxplot to visualize numeric (or quantitative) data, since you can visualize the various distributions of the data.

**Image 3:** Box plot illustration for distribution of data

```{r boxplot, echo=FALSE, fig.align='center'}
ggplot(data = pre_airlinedf, mapping = aes(x = Reporting_Airline, y = ArrDelay, color = Reporting_Airline)) +
  geom_boxplot(fill = "bisque", color = "black", alpha = 0.3) +
  geom_jitter(alpha = 0.2) +
  labs(x = "Airline", y="Arrival Delay Time (mins)") +
  scale_color_manual(values = c("AA" = "red", 
                                 "AS" = "blue", 
                                 "DL" = "green",
                                 "UA" = "orange",
                                 "B6" = "purple",
                                 "PA (1)" = "yellow",
                                 "HP" = "cyan",
                                 "TW" = "magenta",
                                 "VX" = "black"),
                     breaks = c("AA", "AS", "DL", "UA", "B6", "PA (1)", "HP", "TW", "VX"),
                     labels = c("American Airlines", "Alaska Airlines", "Delta Air Lines", "United Airlines", "JetBlue Airways", "Pan American World Airways", "Hawaiian Airlines", "Trans World Airlines", "Virgin America"),
                     name = "Airline") +
  theme_minimal() +
  coord_cartesian(ylim = quantile(pre_airlinedf$ArrDelay, c(0, 0.99)))
```

The above plot tells the distribution of arrival delay time per airlines. In order for us to build a model, we have to check the relationship of the variables we are going to use. Now lets check first the relationship between "DepDelayMinutes" and "ArrDelayMinutes" fields.

**Image 4:** Arrival Delay vs Departure Delay

```{r posscaterplot, echo=FALSE, fig.align='center'}
ggplot(data = pre_airlinedf, mapping = aes(x = DepDelayMinutes, y = ArrDelayMinutes)) +
    geom_point() + 
    geom_smooth(method = "lm") + 
    labs(x = "Departure Delay (Minutes)",
         y = "Arrival Delay (Minutes)")
```

**Table 6:** Correlation value of Deploy Delay to Arrival Delay

```{r correlation1, echo=FALSE}
cor(pre_airlinedf$DepDelayMinutes, pre_airlinedf$ArrDelayMinutes)
```

From the plot, as the depature delay ("DepDelayMinutes") increases, the arrival delay ("ArrDelayMinutes") increases. This indicates a positive direct correlation between these two variables. "DepDelayMinutes" may be a decent predictor of "ArrDelayMinutes" since the regression line is increasing and generally matches the data points. It also has a correlation value of 0.92 denoting strong relationship because the value is nearing 1.

Let's now look at if "WeatherDelay" is a good predictor variable of "ArrDelayMinutes".

**Image 5:** Weather Delay vs Departure Delay

```{r negscaterplot, echo=FALSE, fig.align='center'}
ggplot(data = pre_airlinedf, mapping = aes(x = WeatherDelay, y = ArrDelayMinutes)) +
    geom_point() + 
    geom_smooth(method = "lm") + 
    labs(x = "Weather Delay (Minutes)",
         y = "Arrival Delay (Minutes)")
```

**Table 7:** Correlation value of Weather Delay to Arrival Delay

```{r correlation2, echo=FALSE}
cor(pre_airlinedf$WeatherDelay, pre_airlinedf$ArrDelayMinutes)
```

Weather delay does not seem like a good predictor of arrival delay minutes since the regression line is close to horizontal. Also, for small values of "WeatherDelay", the data points are very scattered and far from the fitted line, showing lots of variability. Therefore it is not a reliable variable.

### Descriptive Statistical Analysis

Here is the descriptive statistical analysis which helps to describe basic features of our dataset and generates a short summary about the sample and measures of the airline data.

**Table 8:** Statistical data summary of the Arrival Delay

```{r statsum, echo=FALSE}
summary_airline_delays <- pre_airlinedf %>%
  group_by(Reporting_Airline) %>%
  summarize(count = n(), 
            mean = mean(ArrDelayMinutes),
            std_dev = sd(ArrDelayMinutes), 
            min = min(ArrDelayMinutes), 
            median = median(ArrDelayMinutes),
            iqr = IQR(ArrDelayMinutes), 
            max = max(ArrDelayMinutes))

summary_airline_delays
```

### Basics of Grouping

Is there any relationship between the reporting airline and the flight delays? If so, which day of the week do flights have relatively longer delay times? For example, people take flights most frequently on Monday and Friday for business trips. Would that impact how long the flight is delayed?

For this section, lets prepare a heatmap to further illustrate the relationship of the day of a week on the delay flights.

**Image 6:** Heatmap for Average Arrival Delays

```{r heatmap, echo=FALSE, message=FALSE, warning=FALSE, fig.align='center'}
# Create avg_delays dataframe as before
avg_delays <- pre_airlinedf %>%
  group_by(Reporting_Airline, DayOfWeek) %>%
  summarize(mean_delays = mean(ArrDelayMinutes), .groups = 'keep') %>%
  mutate(bins = cut(mean_delays,
                    breaks = c(-Inf, 0, 0.1, 10, 20, 30, 50, Inf),
                    labels = c("0", "0-0.1", "0.1-10", "10-20", "20-30", "30-50", ">50"))) %>%
  mutate(bins = factor(bins, levels = c("0", "0-0.1", "0.1-10", "10-20", "20-30", "30-50", ">50")))

# Create a data frame with all possible combinations of Reporting_Airline and DayOfWeek
all_combinations <- expand.grid(Reporting_Airline = unique(avg_delays$Reporting_Airline),
                                DayOfWeek = unique(avg_delays$DayOfWeek))

avg_delays <- merge(avg_delays, all_combinations, by = c("Reporting_Airline", "DayOfWeek"), all = TRUE)
avg_delays[is.na(avg_delays)] <- 0
avg_delays$bins <- factor(avg_delays$bins, levels = c("0", "0-0.1", "0.1-10", "10-20", "20-30", "30-50", ">50"))

ggplot(avg_delays, aes(x = Reporting_Airline, 
                       y = lubridate::wday(DayOfWeek, label = TRUE), 
                       fill = bins)) +
  geom_tile(colour = "white", size = 0.2) +
  geom_text(aes(label = round(mean_delays, 3))) +
  guides(fill = guide_legend(title = "Delays Time Scale")) +
  labs(x = "Reporting Airline", y = "Day of Week") +
  scale_fill_manual(values = c("0" = "#FFFF00", "0-0.1" = "#FFDD00", "0.1-10" = "#FFBB00", "10-20" = "#FF9900", 
                                "20-30" = "#FF7700", "30-50" = "#FF5500", ">50" = "#FF0000"))
```

Given the data, there are airlines that have higher mean of delay flights on every given day of the week.

### Correlation

Correlation is a measure of the extent of interdependence between variables. The main question we are trying to answer in this module is: "What causes flight delays?". To get a better measure of the important characteristics, we look at the correlation of these variables with the arrival delay, AKA, "ArrDelayMinutes", in other words: how is the arrival delay minutes dependent on this variable?

**Image 7:** Correlation Map of the Airline table fields

```{r corr_airline, echo=FALSE, fig.align='center'}
numerics_airline <- pre_airlinedf %>%
  select(ArrDelayMinutes, DepDelayMinutes, CarrierDelay,
         WeatherDelay, NASDelay, SecurityDelay, LateAircraftDelay)

airlines_cor <- cor(numerics_airline, method = "pearson", use='pairwise.complete.obs')

col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))

corrplot(airlines_cor, method = "color", col = col(200),  
         type = "upper", order = "hclust", 
         addCoef.col = "black", # Add coefficient of correlation
         tl.col = "black", tl.srt = 45, #Text label color and rotation
         )
```

From the above correlation plot, you can see that of the features we used, *"CarrierDelay", "DepDelayMinutes", and "LateAircraftDelay"* have the highest correlations with *"ArrDelayMinutes"*. The correlation between *"CarrierDelay"* and "ArrDelayMinutes" is 0.73, *"DepDelayMinutes"* and *"ArrDelayMinutes"* is 0.92, and so on.

### ANOVA (Analysis of Variance)

Let's say that we want to analyze a categorical variable and see the correlation among different categories. For example, consider the airline dataset, the question we may ask is, how do different categories of the reporting airline feature (as a categorical variable) impact flight delays?

The bar graph below shows the average flight delays of different airlines. The code first groups the data by airline, then finds the average arrival delay for each airline, then it plots this as a bar chart.

**Image 8:** Bar chart for the Average Arrival Delays by Airline

```{r bar_per_airline, echo=FALSE, fig.align='center'}
summary_airline_delays <- pre_airlinedf %>%
    group_by(Reporting_Airline) %>%
    summarize(Average_Delays = mean(ArrDelayMinutes, na.rm = TRUE))

summary_airline_delays %>%  
    ggplot(aes(x = Reporting_Airline, y = Average_Delays, fill = Reporting_Airline)) + 
    geom_bar(stat = "identity") +
    scale_fill_manual(values = c("AA" = "red", 
                                 "AS" = "blue", 
                                 "DL" = "green",
                                 "UA" = "orange",
                                 "B6" = "purple",
                                 "PA (1)" = "yellow",
                                 "HP" = "cyan",
                                 "TW" = "magenta",
                                 "VX" = "black"),
                     labels = c("American Airlines", "Alaska Airlines", "Delta Air Lines", "United Airlines", "JetBlue Airways", "Pan American World Airways", "Hawaiian Airlines", "Trans World Airlines", "Virgin America"),
                     name = "Airline")
```

To analyze categorical variables such as the "Reporting_Airline" variable, we can use a method such as the ANOVA method.

**Table 9:** ANOVA results for AA and AS

```{r anova1, echo=FALSE}
aa_as_subset <- pre_airlinedf %>%
  select(ArrDelay, Reporting_Airline) %>%
  filter(Reporting_Airline == 'AA' | Reporting_Airline == 'AS')

ad_aov <- aov(ArrDelay ~ Reporting_Airline, data = aa_as_subset)
summary(ad_aov)
```

In this first example, I compare American Airline and Alaska Airline. The flight delay between "AA" and "AS" are not significantly different, as the F score (0.59) is less than 1 and p-value is larger than 0.05.

**Table 10:** ANOVA results for AA and B6

```{r anova2, echo=FALSE}
aa_b6_subset <- pre_airlinedf %>%
  select(ArrDelay, Reporting_Airline) %>%
  filter(Reporting_Airline == 'AA' | Reporting_Airline == 'B6')

ad_aov <- aov(ArrDelay ~ Reporting_Airline, data = aa_b6_subset)
summary(ad_aov)
```

From the above output, the arrival delay between "AA" and "B6" are significantly different, since the F-score is very large (F = 8.824) and the p-value is 0.0034 which is smaller than 0.05. All in all,there is a strong correlation between a categorical variable and other variables, if the ANOVA test gives a large F-test value and a small p-value.

### Conclusion

Based on the procedures I performed, the variables that are important to take into account when predicting the arrival delay (*ArrDelay*) are the following:

Continuous numerical variables: *DepDelayMinutes CarrierDelay LateAircraftDelay*

Categorical variables: *ReportingAirline*

## Prediction Analysis: Model Development

For this section, the development and assessment of the machine learning model will be performed which will greatly fit the airline dataset for prediction analysis. I used 3 standard models which are:

1.  Simple Linear Regression
2.  Multiple Linear Regression
3.  Polynomial Regression

For Model Development, we will use first the data come from Delta Airlines.

### Simple Linear Regression

Linear Regression is a linear function that predicts the response (dependent) variable as a function of the predictor (independent) variable. Using simple linear regression, the "DepDelayMinutes" will serve as the predictor variable and the "ArrDelayMinutes" as the response variable.

Linear Model Function: \$\$Y\^=18.03865 + (0.88846 \* DepDelayMinutes)\$\$

**Table 11:** Linear Model Summary

```{r lm1, echo=FALSE}
b6_delays <- pre_airlinedf %>%
  filter(CarrierDelay != "NA", Reporting_Airline == "B6")
linear_model <- lm(ArrDelayMinutes ~ DepDelayMinutes, data = b6_delays)
summary(linear_model)
```

**Image 9:** Diagnostic Plots for the Linear Model

```{r lm2, echo=FALSE, fig.align='center', message=FALSE, warning=FALSE}
plot(linear_model)
```

### Multiple Linear Regression

Multiple Linear Regression is very similar to Simple Linear Regression, but this method is used to explain the relationship between one continuous response (dependent) variable and two or more predictor (independent) variables. Using multiple linear regression, the "DepDelayMinutes" and "LateAircraftDelay" will serve as the predictor variable and the "ArrDelayMinutes" as the response variable.

Multiple Linear Model Function: \$\$Y\^=18.03865 + (0.85137 \* DepDelayMinutes) + (0.06992 \* LateAircraftDelay)\$\$

**Table 12:** Multiple Linear Regression Model Summary

```{r mlm1, echo=FALSE}
mlr <- lm(ArrDelayMinutes ~ DepDelayMinutes + LateAircraftDelay, data = b6_delays)
summary(mlr)
```

**Image 10:** Diagnostic Plots for the Multiple Linear Model

```{r mlm2, echo=FALSE, fig.align='center', message=FALSE, warning=FALSE}
plot(mlr)
```

### Polynomial Regression

Polynomial regression is a particular case of the general linear regression model or multiple linear regression models. That is, although the data is nonlinear in polynomial regression (the predicator variables have higher order terms), the model in all cases is linear. The model is always linear because it predicts the coefficients ( b0,b1,... ) which are always of order 1.

Quadratic Polynomial Model Function: \$\$Y\^=18.70939 + (539.976 \* DepDelayMinutes) + (57.483 \* DepDelayMinutes ^ 2)\$\$

**Table 13:** Quadratic Polynomial Regression Model Summary

```{r qpr1, echo=FALSE}
qpr <- lm(ArrDelayMinutes ~ poly(DepDelayMinutes, 2), data = b6_delays)
summary(qpr)
```

**Image 11:** Diagnostic Plots for the Quadratic Polynomial Model

```{r qpr2, echo=FALSE, fig.align='center', message=FALSE, warning=FALSE}
plot(qpr)
```

Cubic Polynomial Model Function: \$\$Y\^=18.70939 + (539.976 \* DepDelayMinutes) + (57.483 \* DepDelayMinutes ^ 2) + (-35.586 \* DepDelayMinutes ^ 3)\$\$

**Table 14:** Cubic Polynomial Regression Model Summary

```{r cpr1, echo=FALSE}
cpr <- lm(ArrDelayMinutes ~ poly(DepDelayMinutes, 3), data = b6_delays)
summary(cpr)
```

**Image 12:** Diagnostic Plots for the Cubic Polynomial Model

```{r cpr2, echo=FALSE, fig.align='center', message=FALSE, warning=FALSE}
plot(cpr)
```

### Assessing the Model

When evaluating our models, not only do we want to visualize the results, but we also want a quantitative measure to determine how accurate the model is. Two very important measures that are often used in Statistics to determine the accuracy of a model are:

R squared, also known as the coefficient of determination, is a measure to indicate how close the data is to the fitted regression line. The value of the R-squared is the percentage of variation of the response variable (y) that is explained by a linear model.

The Mean Squared Error measures the average of the squares of errors, that is, the difference between actual value (y) and the estimated value (ŷ). Another metric that is related to MSE is root mean squared error (RMSE) and is simply the square root of MSE.

**Table 15:** Summary of the Model assessment

```{r assM, echo=FALSE}
# Assuming linear_model and mlr are your linear regression models

# Calculate MSE and RMSE for linear_model
mse_linear <- mean(linear_model$residuals^2)
rmse_linear <- sqrt(mse_linear)
r_squared_linear <- summary(linear_model)$r.squared

# Calculate MSE and RMSE for mlr
mse_mlr <- mean(mlr$residuals^2)
rmse_mlr <- sqrt(mse_mlr)
r_squared_mlr <- summary(mlr)$r.squared

# Calculate MSE and RMSE for qpr
mse_qpr <- mean(qpr$residuals^2)
rmse_qpr <- sqrt(mse_qpr)
r_squared_qpr <- summary(qpr)$r.squared

# Calculate MSE and RMSE for cpr
mse_cpr <- mean(cpr$residuals^2)
rmse_cpr <- sqrt(mse_cpr)
r_squared_cpr <- summary(cpr)$r.squared

# Create a data frame to store the results
results_table <- data.frame(
  Model = c("Linear Model", "Multiple Model", "Quadratic Polynomial Model", "Cubic Polynomial Model"),
  MSE = c(mse_linear, mse_mlr, mse_qpr, mse_cpr),
  RMSE = c(rmse_linear, rmse_mlr, rmse_qpr, rmse_cpr),
  R_squared = c(r_squared_linear, r_squared_mlr, r_squared_qpr, r_squared_cpr)
)

# Print the table
print(results_table)
```

Comparing these three models, the MLR model performs slightly better than the SLR model. Perhaps if we tried adding some more predictor variables, the MLR model could do even better. Of the four models, we conclude that the polynomial of order 3 model seems to be the best fit it as it has the highest R^2 and the lowest MSE.

## Model Evaluation and Refinement

For this section, we are going to do the following steps to evaluate and tune up the data model.

1.  Model Evaluation (Training and Testing Data, Training a Model and Evaluating the Model)
2.  Refinement(Regularization)

### Training and Testing Data

An important step in testing your model is to split the data into training and testing data. The training data will be used to train (fit) models, while the testing data will not be touched until we are evaluating the model. The airline dataset is now divided into 25:75. This means that the proportion of data that is split into the training data is 75% (so the testing data is 25%).

```{r splitdata, include=FALSE}
flight_delays <- pre_airlinedf %>% 
    replace_na(list(CarrierDelay = 0,
                    WeatherDelay = 0,
                    NASDelay = 0,
                    SecurityDelay = 0,
                    LateAircraftDelay = 0)) %>%
    select(c(ArrDelayMinutes, DepDelayMinutes, CarrierDelay, WeatherDelay, NASDelay, SecurityDelay, LateAircraftDelay, DayOfWeek, Month))

set.seed(1234)
flight_split <- initial_split(flight_delays)
train_data <- training(flight_split)
test_data <- testing(flight_split)
```

### Training a Model
After splitting the dataset, the next step is to train the 2 splitted data.

**Table 16:** Fitting results of train and test data
```{r trainmodel, echo=FALSE}
# Pick linear regression
lm_spec <- linear_reg() %>%
  set_engine(engine = "lm")

train_fit4 <- lm_spec %>% 
    fit(ArrDelayMinutes ~  poly(DepDelayMinutes, 3), 
    data = train_data)

train_results4 <- train_fit4 %>%
  predict(new_data = train_data) %>%
  mutate(truth = train_data$ArrDelayMinutes)

test_results4 <- train_fit4 %>%
  predict(new_data = test_data) %>%
  mutate(truth = test_data$ArrDelayMinutes)

print(train_fit4)
```

**Table 17:** Truth table using train data

```{r trainmodel1, echo=FALSE}
head(train_results4)
```

**Table 18:** Truth table using test data

```{r trainmodel2, echo=FALSE}
head(test_results4)
```

To further analyze the testing of the 2 dataset, below is a line plot graph.

**Image 12:** Training results of Test Data Vs. Train Data 
```{r testsplit, echo=FALSE, fig.align='center', message=FALSE, warning=FALSE}
test_results4 %>%
  mutate(train = "testing") %>%
  bind_rows(train_results4 %>% mutate(train = "training")) %>%
  ggplot(aes(truth, .pred)) +
  geom_abline(lty = 2, color = "orange", 
              size = 1.5) +
  geom_point(color = '#006EA1', 
             alpha = 0.5) +
  facet_wrap(~train) +
  labs(x = "Truth", 
       y = "Predicted Arrival Delays (min)")
```

### Evaluating a Model

Next, I evaluated the model. Using metrics RMSE or R2 which are good ways to evaluate regression models.

**Table 19:** Truth table using test data
```{r evalmode, echo=FALSE}
rmse_train4 <- sqrt(mean((train_results4$truth - train_results4$.pred)^2))
rsq_train4 <- rsq(train_results4, truth = truth, estimate = .pred)
rmse_test4 <- sqrt(mean((test_results4$truth - test_results4$.pred)^2))
rsq_test4 <- rsq(test_results4, truth = truth, estimate = .pred)

results_table <- data.frame(
  Metric = c("RMSE (Training)", "R-squared (Training)", "RMSE (Testing)", "R-squared (Testing)"),
  Value = c(rmse_train4, rsq_train4, rmse_test4, rsq_test4)
)

print(results_table)
```

### Regularization

Regularization is a way to handle the problem of overfitting. It is a technique you can use to reduce the complexity of the model by adding a penalty on the different parameters of the model. After it is applied, the model will be less likely to fit the noise of the training data and will improve the generalization abilities of the model. So, regularization is a way of avoiding overfitting by restricting the magnitude of model coefficients.

But for this data train_rmse = test_rmse and it shows a balance approach of testing the two splitted data.

## Prediction

Using the machine learning model, lets now predict the delay time. 

**Image 13:** Average Predicted Delay Time by Airline per Day of Week
```{r prediction, echo=FALSE, message=FALSE, warning=FALSE, fig.align='center'}
airlines <- c("AA", "AS", "DL", "UA", "B6", "PA (1)", "HP", "TW", "VX")
new_data <- pre_airlinedf

predictions <- train_fit4 %>%
  predict(new_data = new_data)

predictions_with_airlines <- cbind(new_data, .pred = predictions)

average_delay <- predictions_with_airlines %>%
  group_by(DayOfWeek, Reporting_Airline) %>%
  summarize(Avg_Predicted_Delay = mean(.pred))

ggplot(average_delay, aes(x = Reporting_Airline, y = Avg_Predicted_Delay, fill = factor(DayOfWeek))) +
  geom_col(position = "dodge") +
  labs(x = "Airline", y = "Average Predicted Delay Time (minutes)") +
  theme_minimal()
```

**Image 14:** Average Predicted Delay Time by Airline per Month
```{r prediction_month, echo=FALSE, message=FALSE, warning=FALSE, fig.align='center'}
airlines <- c("AA", "AS", "DL", "UA", "B6", "PA (1)", "HP", "TW", "VX")
new_data <- pre_airlinedf

# Combine predictions with airline information
predictions_with_airlines <- cbind(new_data, Predicted_ArrDelayMinutes = predictions)

# Calculate average predicted delay time for each airline and month
average_delay <- predictions_with_airlines %>%
  group_by(Month, Reporting_Airline) %>%
  summarize(Avg_Predicted_Delay = mean(.pred))

# Plot the average predicted delay times for each airline and month as a grouped bar chart
ggplot(average_delay, aes(x = Reporting_Airline, y = Avg_Predicted_Delay, fill = factor(Month))) +
  geom_col(position = "dodge") +
  labs(x = "Airline", y = "Average Predicted Delay Time (minutes)") +
  theme_minimal()
```


### Conclusion

Based on the predictive values, Delta Airlines has the highest average predicted delay time during the month of December and also during Sunday of a week. For Amy to make on time on her destination, she needs to take consider the day of the week and the month of the year. If she wants to fly on Monday of June, then she needs to book a flight from Jet Blue Airlines.
